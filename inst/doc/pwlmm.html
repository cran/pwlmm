<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>pwlmm</title>

<script src="data:application/javascript;base64,Ly8gUGFuZG9jIDIuOSBhZGRzIGF0dHJpYnV0ZXMgb24gYm90aCBoZWFkZXIgYW5kIGRpdi4gV2UgcmVtb3ZlIHRoZSBmb3JtZXIgKHRvCi8vIGJlIGNvbXBhdGlibGUgd2l0aCB0aGUgYmVoYXZpb3Igb2YgUGFuZG9jIDwgMi44KS4KZG9jdW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignRE9NQ29udGVudExvYWRlZCcsIGZ1bmN0aW9uKGUpIHsKICB2YXIgaHMgPSBkb2N1bWVudC5xdWVyeVNlbGVjdG9yQWxsKCJkaXYuc2VjdGlvbltjbGFzcyo9J2xldmVsJ10gPiA6Zmlyc3QtY2hpbGQiKTsKICB2YXIgaSwgaCwgYTsKICBmb3IgKGkgPSAwOyBpIDwgaHMubGVuZ3RoOyBpKyspIHsKICAgIGggPSBoc1tpXTsKICAgIGlmICghL15oWzEtNl0kL2kudGVzdChoLnRhZ05hbWUpKSBjb250aW51ZTsgIC8vIGl0IHNob3VsZCBiZSBhIGhlYWRlciBoMS1oNgogICAgYSA9IGguYXR0cmlidXRlczsKICAgIHdoaWxlIChhLmxlbmd0aCA+IDApIGgucmVtb3ZlQXR0cmlidXRlKGFbMF0ubmFtZSk7CiAgfQp9KTsK"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<link rel="stylesheet" href="data:text/css,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" type="text/css" />




</head>

<body>




<h1 class="title toc-ignore">pwlmm</h1>



<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pwlmm)</span></code></pre></div>
<div id="probability-weighted-iterative-generalised-least-squares-for-two-level-multilevel-model-and-two-level-multivariate-multilevel-model" class="section level1">
<h1>Probability Weighted Iterative Generalised Least Squares for Two-level Multilevel Model and Two-level Multivariate Multilevel Model</h1>
<div id="introduction" class="section level2">
<h2>1. Introduction</h2>
<p>Hierarchical models, also known as multilevel models, have been widely applied in analyses where elementary units of a finite population are affected by the social groups or contexts to which they belong to. Examples are: students in schools, patients in hospitals, employees in companies and citizens in municipalities. Such hierarchies, however, need not to be restricted to two levels and can be expanded according to the nature of the population.</p>
<p>Longitudinal data, or repeated measurements data, can be analysed under this approach, as measurements at different time points (occasions) occur within the same individual. Thus, considering the simplest case, a two-level model could be fitted with individuals at the highest level and the series of repeated measurements at the lowest level (Hox et al., 2017). In addition, it may also be of interest to investigate how correlated are the individual measurements at the different points in time. A tool used to deal with this particularity is the multivariate hierarchical model.</p>
<p>Some researchers aim at analysing the results within the scope of those individuals selected for a specific study, while others focus on inferring for the finite population from which the sample was selected. While in the first case the point estimates are made directly from the results of models, for example, in the second case, sampling weights, calculated according to a sampling scheme, should be applied. Sampling schemes may include stratification or clustering of elementary units, giving rise to a complex sample design.</p>
<p>Although the sampling weights are essential for point estimation in survey data, this is often ignored under the hierarchical approach. One of the arguments is that the hierarchical models can incorporate certain characteristics of the sampling scheme as covariates, therefore assuming an ignorable sample design (Rubin, 1976). On the other hand, when the units at any level of the hierarchy are selected with unequal probabilities, this argument may be inappropriate in the sense that the parameter estimates can be asymptotically biased (Pfeffermann et al., 1998) .</p>
<p>Pfeffermann et al. (1998) presented a method for incorporating the sampling weights when fitting two-level random coefficients models. Veiga et al. (2014) presented an extension of these methods, for the multivariate multilevel model in a longitudinal data analysis that incorporates the sampling weights, adapting to the rotating panels of Brazilian labour force survey (BR-LFS).</p>
<p>Thus, the objective of this package is to adapt the methodology presented in Pfeffermann et al. (1998) and in Veiga et al. (2014) under the <span class="math inline">\(R\)</span> environment through improved performance, given the high computational demand arising from the estimation process.</p>
</div>
<div id="estimation-method-for-multilevel-complex-survey-data" class="section level2">
<h2>2. Estimation method for multilevel complex survey data</h2>
<p>Pfeffermann et al. (1998) presented the probability weighted iterative generalized least square (PWIGLS) estimation method base on the already established iterative generalized least square (IGLS) algorithm Goldstein, 2011) for the two-level random coefficients model. In a nutshell, the proposed approach consists of replacing each of the sums of levels 1 and 2 of the original algorithm with weighted sums, where each level weight corresponds to the the inverse of the selection probability at that stage.</p>
<p>Using the framework developed by Pfeffermann et al. (1998), Veiga et al. (2014) extended this method incorporating not only the extra level arising from the longitudinal structure, but also the complex covariance error structure, therefore accommodating the estimation of a multivariate multilevel model. Different error covariance linear structures were accommodated.</p>
<p>The next subsections present some information on the estimation procedure implemented within the package. Please, refer to Pfeffermann et al. (1998), Veiga et al. (2014) for the full description of the theory.</p>
<div id="pwigls-estimation-for-two-level-random-coefficients-models" class="section level3">
<h3>2.1 PWIGLS estimation for two-level random coefficients models</h3>
<p>Let <span class="math inline">\(y_{ij}\)</span>, <span class="math inline">\(\boldsymbol{x}_{ij}\)</span> and <span class="math inline">\(\boldsymbol{z}_{ij}\)</span> be the observed values for the <span class="math inline">\(i\)</span>th level 1 unit within the <span class="math inline">\(j\)</span>th level 2 unit. Therefore, defining <span class="math inline">\(\boldsymbol{y}_j=(y_{1j},...,y_{n_jj})^T\)</span> as a <span class="math inline">\(n_j\times1\)</span> vector, <span class="math inline">\(\boldsymbol{X}_j=(\boldsymbol{x}_{1j},...,\boldsymbol{x}_{n_jj})^T\)</span> as a <span class="math inline">\(n_j\times p\)</span> matrix and <span class="math inline">\(\boldsymbol{Z}_j=(\boldsymbol{z}_{1j},...,\boldsymbol{z}_{n_jj})^T\)</span> as a <span class="math inline">\(n_j\times q\)</span> matrix. The model is represented by <span class="math display">\[
\boldsymbol{y}_{j}=X_{j} \boldsymbol{\beta}+\boldsymbol{r}_{j} \quad \quad \boldsymbol{r}_{j}\sim N(0,\boldsymbol{V}_j)
\]</span> where the composite error is <span class="math inline">\(\boldsymbol{r}_{j}=Z_{j} \boldsymbol{u}_{j}+\boldsymbol{e}_{j}\)</span> and <span class="math inline">\(\boldsymbol{V}_{j}=\boldsymbol{Z}_{j} \Sigma_{u} \boldsymbol{Z}_{j}^{T}+\boldsymbol{I}_{n_{j}} \sigma_{e}^{2}\)</span>, where <span class="math inline">\(\boldsymbol{I}_{n j}\)</span> is an identity matrix size <span class="math inline">\(n_{j} \times n_{j}\)</span>.</p>
<p>For the estimation procedure, <span class="math inline">\(\boldsymbol{V}_{j}\)</span> is expressed as a linear function of <span class="math inline">\(\boldsymbol{\theta}\)</span>, which is the row vector formed with the <span class="math inline">\(s\)</span> distinct elements of <span class="math inline">\(\Sigma_u\)</span> and <span class="math inline">\(\sigma_e\)</span>, such that: <span class="math display">\[\boldsymbol{V}_{j}=\sum_{k=1}^{s} \theta_{k} \boldsymbol{G}_{kj}= \sum_{k=1}^{s} \theta_{k} (  \boldsymbol{Z}_{j} \boldsymbol{H}_{k j} \boldsymbol{Z}_{j}^{T} + \boldsymbol{I}_{n_{j}} \delta_{k s} ).\]</span></p>
<p>Note that, hereafter the notation for sample size, such as <span class="math inline">\(m\)</span> instead of <span class="math inline">\(M\)</span> and <span class="math inline">\(n_j\)</span> instead of <span class="math inline">\(N_j\)</span>, are used.</p>
</div>
<div id="estimation-of-fixed-effects" class="section level3">
<h3>2.2 Estimation of fixed effects</h3>
<p>Fixed effects estimation, considering the general case with <span class="math inline">\(q \geq 1\)</span>, where <span class="math inline">\(q\)</span> is the number of random effects at level two, is taken by <span class="math display">\[\boldsymbol{\hat{\beta}}^{(r)} = \boldsymbol{\hat{P}}^{(r) {-1}} \boldsymbol{\hat{q}}^{(r)}.\]</span> Given that <span class="math display">\[\boldsymbol{V}_{jr}^{-1}=\hat{\sigma}^{-2}_e \boldsymbol{D}_{j}^{-1}-\hat{\sigma}^{-2}_e \boldsymbol{D}_{j}^{-1} \boldsymbol{Z}_{j} \hat{\boldsymbol{A}}_j \boldsymbol{Z}_{j}^{T} \boldsymbol{D}_{j}^{-1}\]</span> is the inverse of <span class="math inline">\(\boldsymbol{V}_j\left(\boldsymbol{\hat{\theta}}^{(r-1)}\right)\)</span>, where <span class="math inline">\(r\)</span> is for the iteration 1, 2, , and <span class="math display">\[\hat{\boldsymbol{A}}_j=(\boldsymbol{Z}_j^T\boldsymbol{D}_j^{-1}\boldsymbol{Z}_j +\hat{\sigma}_e^2\hat{\boldsymbol{\Sigma}}_u^{-1})^{-1}\]</span> we can write <span class="math display">\[\begin{align} \tag{1} \label{P est}
\boldsymbol{\hat{P}}^{(r)} &amp;=\sum_j w_j (X_j^{T}\boldsymbol{V}_{jr}^{-1}X_j) \\ &amp;=\sum_{j=1}^{m}{w_j(\boldsymbol{\hat{T}}_{1j}-\boldsymbol{\hat{T}}_{2j} \hat{\boldsymbol{A}}_j \boldsymbol{\hat{T}}_{2j}^T)},
 \end{align}\]</span> and <span class="math display">\[\begin{align} \tag{2} \label{q est}
  \boldsymbol{\hat{q}}^{(r)}&amp;=\sum_j w_j  (X_j^{T}\boldsymbol{V}_{jr}^{-1}\boldsymbol{y}_j) \\
  &amp; =\sum_{j=1}^{m}{w_j(\boldsymbol{\hat{t}}_{3j}-\boldsymbol{\hat{T}}_{2j} \hat{\boldsymbol{A}_j} \boldsymbol{\hat{t}}_{4j})},
 \end{align}\]</span> where <span class="math inline">\(\boldsymbol{D}_j^{-1}=\text{diag}(w_{i|j})\)</span> is a <span class="math inline">\(n_j\times n_j\)</span> diagonal matrix with <span class="math inline">\(w_{i|j}\)</span> in the main diagonal, <span class="math inline">\(\boldsymbol{\hat{T}}_{1j}=\boldsymbol{X}_j^T\boldsymbol{D}_j^{-1}\boldsymbol{X}_j\)</span>, <span class="math inline">\(\boldsymbol{\hat{T}}_{2j}=\boldsymbol{X}_j^T\boldsymbol{D}_j^{-1}\boldsymbol{Z}_j\)</span>, <span class="math inline">\(\boldsymbol{\hat{t}}_{3j}=\boldsymbol{X}_j^T\boldsymbol{D}_j^{-1}\boldsymbol{y}_j\)</span>, <span class="math inline">\(\boldsymbol{\hat{t}}_{4j}=\boldsymbol{Z}_j^T\boldsymbol{D}_j^{-1}\boldsymbol{y}_j\)</span>, <span class="math inline">\(\boldsymbol{\hat{T}_{5j}}=\boldsymbol{Z}_j^T\boldsymbol{D}_j^{-1}\boldsymbol{Z}_j\)</span>, and re-writing <span class="math inline">\(\hat{\boldsymbol{A}}_j=(\boldsymbol{\hat{T}_{5j}}+\hat{\sigma}_e^2\hat{\boldsymbol{\Sigma}}_u^{-1})^{-1}\)</span>.</p>
</div>
<div id="estimation-of-random-effects" class="section level3">
<h3>2.3 Estimation of Random effects</h3>
<p>Taking the random part of the model to be represented by <span class="math inline">\(\boldsymbol{\theta}\)</span>, the estimation of the random effects <span class="math inline">\(\boldsymbol{\hat{\theta}}^{(r)}\)</span> follows the matrix multiplication: <span class="math display">\[\boldsymbol{\hat{\theta}}^{(r)}=\boldsymbol{\hat{R}}^{(r) {-1}} \boldsymbol{\hat{s}}^{(r)}.\]</span></p>
<p>The representation for both <span class="math inline">\(\boldsymbol{\hat{R}}^{(r)}\)</span> and <span class="math inline">\(\boldsymbol{\hat{s}}^{(r)}\)</span> sums are defined by expressions for each entry in the matrix/vector. We can write <span class="math display">\[\begin{align} \tag{3}
\label{Rr est}
\boldsymbol{\hat{R}}^{(r)} &amp;= \sum_j w_j tr(\boldsymbol{V}_{jr}^{-1}\boldsymbol{G}_{kj} \boldsymbol{V}_{jr}^{-1}G_{lj}) \\
&amp; = \sum_{j=1}^{m} \boldsymbol{\hat{T}}_{rj}^{(r)},
\end{align}\]</span> and <span class="math display">\[\begin{align} \tag{4}
\label{Ss est}
\boldsymbol{\hat{s}}^{(r)}&amp; =\sum_j w_j tr(\boldsymbol{\hat{e}}_j^T\boldsymbol{V}_{jr}^{-1}\boldsymbol{G}_{kj}\boldsymbol{V}_{jr}^{-1}\boldsymbol{\hat{e}}_{j}) \\
&amp;= \sum_{j=1}^{m} \boldsymbol{\hat{t}}_{sj}^{(r)},
\end{align}\]</span> where <span class="math inline">\(\boldsymbol{\hat{T}}_{rj}^{(r)}\)</span>is a <span class="math inline">\(s\times s\)</span> matrix, where <span class="math inline">\(s\)</span> is the total number of parameters in <span class="math inline">\(\boldsymbol{\theta}\)</span>, and the <span class="math inline">\(kl\)</span>th element is expressed as <span class="math display">\[\begin{equation} \label{R est}
w_j\left\{\delta_{ks}\delta_{ls}\hat{N}_j + \delta_{ls}\mbox{tr}\left(\boldsymbol{Z}_j^T\boldsymbol{D}_j^{-1}\boldsymbol{Z}_j\hat{\boldsymbol{C}}_{kj}\right)+ \delta_{ks}\mbox{tr}\left(\boldsymbol{Z}_j^T\boldsymbol{D}_j^{-1}\boldsymbol{Z}_j\boldsymbol{H}_{l}\right)+
\mbox{tr}\left(\boldsymbol{Z}_j^T\boldsymbol{D}_j^{-1}\boldsymbol{Z}_j\hat{\boldsymbol{C}}_{kj}\boldsymbol{Z}_j^T\boldsymbol{D}_j^{-1}\boldsymbol{Z}_j\boldsymbol{H}_{l}\right)\right\}
\end{equation}\]</span> and <span class="math inline">\(\boldsymbol{\hat{t}}_{sj}^{(r)}\)</span> is a vector of length <span class="math inline">\(s\)</span> and the <span class="math inline">\(k\)</span>th element is <span class="math display">\[\begin{equation} \label{S est}
w_j\left[\delta_{ks}\mbox{tr}\left\{\hat{\boldsymbol{e}}_j ^T\boldsymbol{D}_j^{-1}\hat{\boldsymbol{e}}_j  \right\}+\mbox{tr}\left\{\hat{\boldsymbol{e}}_j ^T\boldsymbol{D}_j^{-1}\boldsymbol{Z}_j\hat{\boldsymbol{C}}_{kj}\boldsymbol{Z}_j^T\boldsymbol{D}_j^{-1}\hat{\boldsymbol{e}}_j  \right\} \right]
\end{equation}\]</span> where <span class="math inline">\(\hat{\boldsymbol{e}}_j =\left(\boldsymbol{y}_j-\boldsymbol{X}_j\boldsymbol{\hat{\beta}}^{(r)}\right)\)</span>, <span class="math inline">\(\delta_{ks}=1\)</span> if <span class="math inline">\(k=s\)</span> (0, otherwise), <span class="math inline">\(\hat{N}_j=\sum_{i=1}^{n_j}w_{i|j}\)</span>, <span class="math inline">\(\hat{\boldsymbol{C}}_{kj}= -\delta_{ks}\hat{\boldsymbol{A}}_j+\hat{\boldsymbol{B}}_{kj}-\hat{\boldsymbol{B}}_{kj}\boldsymbol{\hat{T}_{5j}}\hat{\boldsymbol{A}}_j\)</span>, <span class="math inline">\(\hat{\boldsymbol{B}}_{kj}=\hat{\sigma}_e^2\hat{\boldsymbol{A}}_j\hat{\boldsymbol{\Sigma}}_u^{-1}\boldsymbol{H}_{k}-\delta_{ks}\hat{\boldsymbol{A}}_j\)</span>, <span class="math inline">\(\boldsymbol{H}_{k}\)</span> (<span class="math inline">\(k = 1 ... s\)</span>) is a known <span class="math inline">\(q\times q\)</span> matrix, and the <span class="math inline">\(ab\)</span>th element of this matrix is equal to 1 if the <span class="math inline">\(ab\)</span>th entry in <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_u\)</span> corresponds to <span class="math inline">\(\hat{\theta}_k\)</span>(0, otherwise).</p>
</div>
<div id="initial-estimates" class="section level3">
<h3>2.4 Initial estimates</h3>
<p>Initial values for <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> is given by <span class="math display">\[\begin{equation} \label{beta}
\boldsymbol{\hat{\beta}}^{(0)}=\left(\sum_{j=1}^{m} w_j\boldsymbol{\hat{T}}_{1j} \right)^{-1} \left(\sum_{j=1}^{m}w_j \boldsymbol{\hat{t}}_{3j}\right).
\end{equation}\]</span></p>
<p>In order to calculate the initial estimate for <span class="math inline">\(\boldsymbol{\hat{\theta}}\)</span>, we have to split this parameter into level 2 and level 1 random effects, in order to get <span class="math inline">\(\boldsymbol{\Sigma}_u\)</span> and <span class="math inline">\(\sigma_e^2\)</span> respectively. That enables the initial estimate to be applied separately to each part. With respect to <span class="math inline">\(\boldsymbol{\Sigma}_u\)</span>, the diagonal entries are filled by <span class="math inline">\(0.5\)</span>, which represents the level 2 variances, while the level 2 covariance(s) are represented by zero(s). The level 1 residual <span class="math inline">\(\sigma_e^2\)</span> initial value is given by <span class="math display">\[\begin{equation}
\hat{\sigma}_e^{(0)2}=\frac{\sum_{j=1}^{m} w_j\hat{T}_{6j}^{(0)}}{\sum_{j=1}^{m} w_j(\hat{N}_j-1)}
\end{equation}\]</span> where <span class="math inline">\(\hat{T}_{6j}^{(0)}=\boldsymbol{w}_{i|j}^T(\hat{\boldsymbol{e}}_j^{(0)}-\boldsymbol{\hat{u}_j}^{(0)})^2\)</span>, <span class="math inline">\(\hat{\boldsymbol{e}}_j^{(0)}=\boldsymbol{y}_j-\boldsymbol{X}_j\hat{\boldsymbol{\beta}}^{(0)}\)</span> and <span class="math inline">\(\boldsymbol{\hat{u}_j}^{(0)}\)</span>is a <span class="math inline">\(n_j\times1\)</span> vector of repeated <span class="math inline">\(\hat{u}_j^{(0)}=\frac{\boldsymbol{w}_{i|j}^T\boldsymbol{e}_j^{(0)}}{\sum_{i=1}^{n_j}w_{i|j}}\)</span>.</p>
</div>
<div id="variances" class="section level3">
<h3>2.5 Variances</h3>
<p>The variance estimators of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> can be expressed as <span class="math display">\[\begin{equation} \label{vbeta}
\widehat{Var}(\boldsymbol{\hat{\beta}})=\boldsymbol{\hat{P}}^{-1}\left(\frac{m}{m-1}\right) \left(\sum_{j=1}^{m}w_j^2\boldsymbol{c}_j\boldsymbol{c}_j^T\right)\boldsymbol{\hat{P}}^{-1},
\end{equation}\]</span> where <span class="math inline">\(\boldsymbol{\hat{P}}=lim_{r\to \infty}\boldsymbol{\hat{P}}^{(r)}\)</span>, <span class="math inline">\(\boldsymbol{c}_j=\boldsymbol{X}_j^T\boldsymbol{D}_j^{-1}\boldsymbol{e}_j - \boldsymbol{\hat{T}}_{2j} \hat{\boldsymbol{A}}_j \boldsymbol{Z}_j^T\boldsymbol{D}_j^{-1}\boldsymbol{e}_j\)</span> and <span class="math inline">\(\boldsymbol{e}_j=\boldsymbol{y}_j-\boldsymbol{X}_j \boldsymbol{\hat{\beta}}\)</span>. The variance estimator of <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> can be expressed as <span class="math display">\[\begin{equation} \label{vtheta}
\widehat{Var}(\boldsymbol{\hat{\theta}})=\boldsymbol{\hat{R}}^{-1}\left(\frac{m}{m-1}\right) \left(\sum_{j=1}^{m}w_j^2\boldsymbol{d}_j\boldsymbol{d}_j^T\right)\boldsymbol{\hat{R}}^{-1},
\end{equation}\]</span> where <span class="math inline">\(\boldsymbol{\hat{R}}=lim_{r\to \infty}\boldsymbol{\hat{R}}^{(r)}\)</span>, <span class="math inline">\(\boldsymbol{d}_j=\boldsymbol{\hat{t}}_{sj}-\boldsymbol{\hat{T}}_{rj} \boldsymbol{\hat{\theta}}\)</span>, <span class="math inline">\(\boldsymbol{\hat{t}}_{sj}=lim_{r\to \infty}\boldsymbol{\hat{t}}_{sj}^{(r)}\)</span>and <span class="math inline">\(\boldsymbol{\hat{T}}_{rj}=lim_{r\to \infty}\boldsymbol{\hat{T}}_{rj}^{(r)}\)</span>.</p>
</div>
<div id="residuals" class="section level3">
<h3>2.6 Residuals</h3>
<p>The level 2 residuals are estimated according to equations and expressions from Appendix 2.2 in Goldstein (2011) . The estimated residuals at level <span class="math inline">\(h\)</span> (here the level 2) for the <span class="math inline">\(j\)</span>th group is given by <span class="math display">\[\begin{equation} \label{level2res}
\hat{\boldsymbol{u}}_j=\boldsymbol{R}_{hj}^T \boldsymbol{V}^{-1}_j \boldsymbol{e}_j,
\end{equation}\]</span> where <span class="math inline">\(\boldsymbol{R}_{hj}=\boldsymbol{Z}_j\hat{\boldsymbol{\Sigma}}_u\)</span> and <span class="math inline">\(\boldsymbol{V}_j=\boldsymbol{R}_{hj}\boldsymbol{Z}^T_j+\hat{\sigma}_e^2\boldsymbol{I}_{n_j}\)</span>. The variance estimator of <span class="math inline">\(\boldsymbol{\hat{u}_j}\)</span> is <span class="math display">\[\begin{equation} \label{varlevel2res}
\widehat{Var}(\boldsymbol{\hat{u}_j})=\mbox{diag}(\hat{\boldsymbol{\Sigma}}_u-\boldsymbol{R}_{hj}^T \boldsymbol{V}^{-1}_j \boldsymbol{R}_{hj}).
\end{equation}\]</span></p>
</div>
<div id="scaled-weights" class="section level3">
<h3>2.7 Scaled weights</h3>
<p>In a two-stage sampling scheme, level 2 units are selected with sampling probability <span class="math inline">\(\pi_j\)</span>; at the second stage, level 1 units are selected within the <span class="math inline">\(j\)</span>th level 2 unit with probability <span class="math inline">\(\pi_{i|j}\)</span>. The sampling weights used in the PWIGLS method are defined as <span class="math display">\[w_j=\pi_j^{-1}\]</span> and <span class="math display">\[w_{i|j}=\pi_{i|j}^{-1}\]</span>.</p>
<p>In order to reduce sample bias, it is essential to scale the weights <span class="math inline">\(w_{j}\)</span> and <span class="math inline">\(w_{i|j}\)</span>. Pfeffermann et al. (1998) suggested their method called scale method 2, therefore transforming <span class="math inline">\(w_{j}\)</span> and <span class="math inline">\(w_{i|j}\)</span> respectively to <span class="math display">\[\begin{equation} \label{wj}
w_{j}^*=\frac{w_j}{\tilde{w}}
\end{equation}\]</span> and <span class="math display">\[\begin{equation} \label{wij}
w_{i|j}^*=\frac{w_{i|j}}{\tilde{w}_j}
\end{equation}\]</span> where <span class="math inline">\(\tilde{w}=\sum_{j=1}^m w_j/m\)</span> and <span class="math inline">\(\tilde{w}_j=\sum_{i=1}^{n_j} w_{i|j}/n_j\)</span>. Hence, <span class="math inline">\(\hat{N}_j\)</span> becomes <span class="math inline">\(n_j\)</span> in equation <span class="math inline">\(3\)</span>, as explained by Pfeffermann et al. (1998).</p>
<p>The package applies this scaling method.</p>
</div>
</div>
<div id="pwigls-estimation-for-multivariate-multilevel-models" class="section level2">
<h2>3. PWIGLS Estimation for multivariate multilevel models</h2>
<p>Now, let the model be represented by <span class="math display">\[
\boldsymbol{Y}_{j}=X_{j} \boldsymbol{\beta}+\boldsymbol{r}_{j} \quad \quad \boldsymbol{r}_{j} \sim N\left(0, \boldsymbol{V}_{j}\right)
\]</span> where <span class="math inline">\({r}_{j}\)</span> is defined as follows, <span class="math display">\[\begin{equation}
\label{eq:rescomp}
r_{tij} = v_j + d_{tij} u_{ij}.   
\end{equation}\]</span></p>
The estimation process for the multivariate multilevel model with weights is fully described in Veiga et al. (2014). It is a similar process as the PWIGLS for the random coefficients model described above. The differences are in the definition of: <span class="math display">\[\begin{equation} \label{deltamm} \boldsymbol{V}_j= \sum_{k=1}^s \theta_k \boldsymbol{G}_{kj} = \sum_{k=1}^s \theta_k ( \boldsymbol{1} \boldsymbol{H}_{kj} \boldsymbol{1}^T +  \boldsymbol{I}_{n_j}\otimes \Delta_{kj}),\end{equation}\]</span> therefore, changes are observed in:
<p>Furthermore, we can write <span class="math display">\[\hat{\boldsymbol{A}}_j =\left\{ \hat{\Sigma}_v^{-1} +  \boldsymbol{Z}_j^{T} (\boldsymbol{W}_{j}\otimes \hat{\Sigma}_u^{-1}) \boldsymbol{Z}_j \right\}^{-1} \;\]</span> and <span class="math display">\[\hat{\boldsymbol{V}}_{jr}^{-1} = \boldsymbol{W}_{j}\otimes \hat{\Sigma}_u^{-1} - (\boldsymbol{W}_{j}\otimes \hat{\Sigma}_u^{-1})\boldsymbol{Z}_j \hat{\boldsymbol{A}}_j \boldsymbol{Z}_j^{T}(\boldsymbol{W}_{j}\otimes \hat{\Sigma}_u^{-1}).\]</span> The fixed effects are then, as previously, estimated by solving the equations <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> and the random part of the model is estimated by solving the equations <span class="math inline">\(3\)</span> and <span class="math inline">\(4\)</span>.</p>
</div>
<div id="functions" class="section level2">
<h2>4. Functions</h2>
<p>The format adopted for the estimation of two-level hierarchical linear models with weights was based in the same format used by Bates et al. (2015) package, <code>lme4</code>. The format for the multivariate hierarchical linear models with weights was based on the basic functions of <span class="math inline">\(R\)</span> for the estimation of linear models (<code>lm</code>, <code>glm</code>). The main difference in format between the two is that in the first, the use of random coefficients at the cluster level is allowed.</p>
<p>For estimating two-level linear models with weights, the command required is</p>
<pre><code>pwigls2 (formula, data = NULL, wj, wi_j)</code></pre>
<p>where <code>pwigls2</code> is the name of the function; <code>formula</code> is the formula where the response and explanatory variables are placed in such as <code>Y = X_1+ X_2 + ... + X_p + (1 | group)</code>, so that the variables with random effects are between the parentheses to the left of the vertical bar and the group identifier variable to the right; <code>data</code> is a <code>data frame</code> (optional) containing the variables used in <code>formula</code>; <code>wj</code> is the vector of weights corresponding to level 2 units, and <code>wi_j</code> is the vector of weights corresponding to level 1 units, conditional to their respective level 2 unit.</p>
<p>For estimating multivariate multilevel linear models with weights, the command required is</p>
<pre><code>wmlmm(formula, data = NULL, ID3, ID2, ID1, wj, wi_j, type, rot = NULL) </code></pre>
<p>where <code>wmlmm</code> is the name of the function; <code>formula</code> is the formula where the response and explanatory variables are placed such as <code>Y = X_1 + X_2 + ... + X_p</code>. There is no need to reference the <span class="math inline">\(k\)</span> points as explanatory variables; <code>ID3</code>, <code>ID2</code> and <code>ID1</code> are the identifiers for level 2 units, level 1 units, and time-dummies for level 1 units, respectively; <code>type</code> is the type of error covariance structure that the user wants to estimate, that can be: <code>toep</code>, for Toeplitz; <code>uns</code> for unstructured; and <code>genlin</code> for the general linear with dependency on <code>lag</code>; <code>rot</code> is a vector of 0’s and 1’s that identifies the occasions to automatically create the matrices <span class="math inline">\(A_1, \dots, A_k\)</span> of <span class="math display">\[\begin{equation} \label{genlin}
    \Sigma^{genlin}_r = \theta_1 A_1 + \theta_2 A_2 + \cdots + \theta_t A_k,
\end{equation}\]</span> for <code>type = genlin</code> (see example below); and <code>data</code> and <code>wj</code> have the same functionality here as in the function <code>pwigls2</code>, while <code>wi_j</code>, for a longitudinal data, corresponds to the longitudinal weights.</p>
</div>
<div id="references" class="section level2">
<h2>5. References</h2>
<p>Bates, D., M ̈achler, M., Bolker, B., and Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1):1–48</p>
<p>Goldstein, H. (2011). Multilevel statistical models, volume 922. John Wiley &amp; Sons.</p>
<p>Hox, J. J., Moerbeek, M., and Van de Schoot, R. (2017). Multilevel analysis: Techniques and applications. Routledge.</p>
<p>Pfeffermann, D., Skinner, C. J., Holmes, D. J., Goldstein, H., and Rasbash, J. (1998). Weighting for unequal selection probabilities in multilevel models. Journal of the Royal Statistical Society: series B (statistical methodology), 60(1):23–40.</p>
<p>Veiga, A., Smith, P. W., and Brown, J. J. (2014). The use of sample weights in multivariate multilevel models with an application to income data collected by using a rotating panel survey. Journal of the Royal Statistical Society: Series C (Applied Statistics), 63(1):65–84</p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
